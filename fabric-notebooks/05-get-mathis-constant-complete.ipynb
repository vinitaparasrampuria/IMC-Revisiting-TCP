{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02a5be90-977a-4948-a299-b5df71273eef",
   "metadata": {},
   "source": [
    "# Run experiments to \"Validate Mathis model at Edge, Core and Intermediate Scale\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d083f0-5028-4517-8d4e-eba13f28cac4",
   "metadata": {},
   "source": [
    "## Set up your FABRIC environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf58277-480d-4459-a9c6-08a085b1313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
    "fablib = fablib_manager() \n",
    "fablib.show_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccb86ad-4f11-4dcd-acf8-748361d615c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 600 {fablib.get_bastion_key_filename()}\n",
    "!chmod 600 {fablib.get_default_slice_private_key_file()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a288b3d-4162-4249-8c9a-111ddb4beecf",
   "metadata": {},
   "source": [
    "## Get slice details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7704d48-a158-4c40-ba58-9ea4f462706c",
   "metadata": {},
   "source": [
    "Put your slice name and the number of endpoints in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3047eaf-4fb4-40c1-8dd0-c2f364685384",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_endpoints = 10\n",
    "slice_name=\"bottleneck-\" + str(n_endpoints) + '-test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e8f83-c35a-4669-98b5-647ef965efb1",
   "metadata": {},
   "source": [
    "Then, load your slice details into the environment.slice = fablib.new_slice(name=slice_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9af4c-28f9-4314-8791-878b4c41d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = fablib.get_slice(name=slice_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac3a090-f28c-4665-a470-06a9967decac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_nodes = [slice.get_node(name='sender-' + str(i))  for i in range(n_endpoints)]\n",
    "receiver_nodes = [slice.get_node(name='receiver-' + str(i))  for i in range(n_endpoints)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7db1cb-7e04-4077-bcd2-c135b78d8cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_node = slice.get_node(name='router')\n",
    "router_ingress_iface = router_node.get_interface(network_name = \"link-sender\")\n",
    "router_egress_iface  = router_node.get_interface(network_name = \"link-receiver\")\n",
    "router_egress_name = router_egress_iface.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43392b9f-43fc-4f4a-9734-f63bc99a882f",
   "metadata": {},
   "source": [
    "## Setup the scripts and modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb577e03-6a84-4edd-b340-6e6bc742a992",
   "metadata": {},
   "source": [
    "First, we will upload all the scripts we need to analyze the results for this experiment. This needs to be done only once before start of experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fcb13-840f-4ccd-b8b3-12d48bee1feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in sender_nodes:\n",
    "    n.upload_file('process_cwn_file.py','process_cwn_file.py')\n",
    "    n.upload_file('process_iperf_normal.py','process_iperf_normal.py')\n",
    "    n.upload_file('mathis_sender.py','mathis_sender.py')\n",
    "    n.upload_file(\"cwn.sh\", \"cwn.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f66e6a-1ccb-4efb-b794-7f3f65ab925d",
   "metadata": {},
   "source": [
    "Also we need sklearn to process the downloaded files, so install the library in the environment.This needs to be done only once before start of experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3dcbb-afdc-464e-8e40-fac44e628586",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08443dd-e807-4b6d-9346-78d59c6352c8",
   "metadata": {},
   "source": [
    "## Generate flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a55ca7d-058b-4974-ba1e-47c074d9f50e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set experiment parameters\n",
    "\n",
    ">cca, delay, test_duration, num_servers, flows, interval, omit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b034aa90-b05f-4b1f-93d7-c7475100a6cb",
   "metadata": {},
   "source": [
    "cca is the congestion control algorithm (reno for this experiment)\n",
    "\n",
    "delay is the delay to be set at the receiver (20 ms,100 ms,200 ms)\n",
    "\n",
    "test_duration is the time for which to send the iperf3 flows (10800 is used for these experiments).\n",
    "\n",
    "num_servers is the number of ports to be opened on each receiver. \n",
    "CoreScale : 100,3000 and 500 ports\n",
    "EdgeScale : 1,3 and 5 ports\n",
    "IntermediateScale: 10, 30 and 50 ports\n",
    "\n",
    "flows is the number of parallel flows to be send from each port. It is set to 1 for all the experiments.\n",
    "\n",
    "interval is the periodic time interval to save the result by iperf3.\n",
    "\n",
    "omit is the starting n seconds to ignore the iperf values.(set to 0 for all the experiments. Max value=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba875fc8-39b8-4cd2-a677-6ec5eb2f5643",
   "metadata": {},
   "outputs": [],
   "source": [
    "cca=\"reno\"\n",
    "delay=200\n",
    "test_duration=3600\n",
    "num_servers=100\n",
    "flows=1\n",
    "interval=0.01\n",
    "omit=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c690bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate full factorial experiment\n",
    "import itertools\n",
    "exp_factors_core = {\n",
    "    'scenario': ['core'], \n",
    "    'rate': ['10Gbit'],\n",
    "    'limit': ['375MB'],\n",
    "    'cca': [\"reno\"],\n",
    "    'delay': [20],\n",
    "    'test_duration': [600],\n",
    "    'num_servers': [100, 300, 500],\n",
    "    'flows': [1],\n",
    "    'interval': [0.01],\n",
    "    'omit': [0],\n",
    "    'trial': [1]\n",
    "}\n",
    "factor_names = [k for k in exp_factors_core]\n",
    "factor_lists = list(itertools.product(*exp_factors_core.values()))\n",
    "exp_lists_core = [dict(zip(factor_names, factor_l)) for factor_l in factor_lists]\n",
    "\n",
    "exp_factors_edge = { \n",
    "    'scenario': ['edge'], \n",
    "    'rate': ['100Mbit'],\n",
    "    'limit': ['3MB'],\n",
    "    'cca': [\"reno\"],\n",
    "    'delay': [20],\n",
    "    'test_duration': [600],\n",
    "    'num_servers': [100, 300, 500],\n",
    "    'flows': [1],\n",
    "    'interval': [0.01],\n",
    "    'omit': [0],\n",
    "    'trial': [1]\n",
    "}\n",
    "factor_names = [k for k in exp_factors_edge]\n",
    "factor_lists = list(itertools.product(*exp_factors_edge.values()))\n",
    "exp_lists_edge = [dict(zip(factor_names, factor_l)) for factor_l in factor_lists]\n",
    "\n",
    "exp_lists = exp_lists_core + exp_lists_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dd390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/\"\n",
    "# TODO add code: if directory does not exist, make it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a9b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_node = slice.get_node(name='router')\n",
    "router_ingress_iface = router_node.get_interface(network_name = \"link-sender\")\n",
    "router_egress_iface  = router_node.get_interface(network_name = \"link-receiver\")\n",
    "router_egress_name = router_egress_iface.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d225edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # to allow resume\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.backends.backend_pdf\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import sys\n",
    "import os\n",
    "for exp in exp_lists:\n",
    "\n",
    "    # check if we already ran this experiment\n",
    "    # (allow stop/resume)\n",
    "    exp_name_str = \"_\".join( [str(v) for v in exp.values()] )\n",
    "    c_file_out = data_dir + 'c_' + \"_\".join( [str(v) for v in exp.values()] ) # file with mathis constant\n",
    "    j_file_out = data_dir + 'j_' + \"_\".join( [str(v) for v in exp.values()] ) # file with JFI\n",
    "\n",
    "    # TODO check if the c file and j file already exist\n",
    "    if file_exists:\n",
    "        print(\"Already have \" + c_file_out + \" and \" + j_file_out \", skipping\")\n",
    "\n",
    "    else:\n",
    "        print(\"Running experiment to generate \" +  c_file_out + \" and \" + j_file_out )\n",
    "\n",
    "\n",
    "        # set up edge or core scale setting\n",
    "        # first delete any existing queue\n",
    "        router_node.execute(\"sudo tc qdisc del dev \" + router_egress_name + \" root\")\n",
    "        # then set one up with HW offload\n",
    "        router_node.execute(\"sudo tc qdisc replace dev \" + router_egress_name + \" root handle 1: htb default 3 offload\")\n",
    "        router_node.execute(\"sudo tc class add dev \" + router_egress_name + \" parent 1: classid 1:3 htb rate \" + exp['rate'])\n",
    "        router_node.execute(\"sudo tc qdisc add dev \" + router_egress_name + \" parent 1:3 handle 3: bfifo limit \" + exp['limit'])\n",
    "\n",
    "        # TODO - move the experiment exec procedure inside the loop\n",
    "\n",
    "        ## Get queue statistics on the router before experiment\n",
    "        router_node.execute(\"tc -p -s -d -j qdisc show dev \"+router_egress_name +\" >tc_before\"+exp_name_str+\".txt\")\n",
    "\n",
    "        ## Remove existing result files from the hosts #Check if the files are removed from the senders and receivers\n",
    "        for n in receiver_nodes:\n",
    "            n.execute(\"rm -f 60*\")\n",
    "        for n in sender_nodes:\n",
    "            n.execute(\"rm -f sender*\")\n",
    "            n.execute(\"rm -f data*\")\n",
    "            n.execute(\"rm -f packet*\")\n",
    "            n.execute(\"rm -f output*\")\n",
    "\n",
    "        #Now set up delay on the receiver interface:\n",
    "        #First delete any existing queue (don't worry if there is an error, it means there was not!)\n",
    "        for n in receiver_nodes:\n",
    "            receiver_inf=n.get_interface(network_name= \"link-receiver\")\n",
    "            receiver_inf_name = receiver_inf.get_device_name()\n",
    "            n.execute(\"sudo tc qdisc del dev \" + receiver_inf_name + \" root netem\")\n",
    "            n.execute(\"sudo tc qdisc add dev \" + receiver_inf_name + \" root netem delay \" + str(exp['delay']) +\" ms limit 1000000\")\n",
    "        \n",
    "        ## Start parallel servers on the receiver\n",
    "            \n",
    "        for i, n in enumerate(receiver_nodes):\n",
    "            n.execute(\"sudo killall iperf3\")\n",
    "            n.execute_thread(f'chmod +x iperf-parallel-servers.sh && bash iperf-parallel-servers.sh '+str(exp['num_servers']))\n",
    "\n",
    "        #check if the required number of ports are opened\n",
    "        for n in receiver_nodes:\n",
    "            n.execute(\"ls -1 | wc -l\")\n",
    "        \n",
    "\n",
    "        ## Start parallel clients on the sender\n",
    "        for i, n in enumerate(sender_nodes):\n",
    "            n.execute(\"sudo killall iperf3\")\n",
    "            n.execute_thread(f'chmod +x iperf-parallel-senders.sh && bash iperf-parallel-senders.sh 10.10.2.1'+str(i)+\" \"+str(exp['num_servers']+\" \"+str(exp['test_duration'])+\" \"+exp['cca']+\" \"+str(exp['flows'])+\" \"+str(exp['interval'])+\" \"+str(exp['omit']))\n",
    "            n.execute_thread(f'chmod +x cwn.sh && bash cwn.sh 10.10.2.1'+str(i))\n",
    "        time.sleep(exp['test_duration']+300) \n",
    "\n",
    "        ## Get queue statistics on the router after experiment\n",
    "        router_node.execute(\"tc -p -s -d -j qdisc show dev \"+router_egress_name +\" >tc_before\"+exp_name_str+\".txt\")\n",
    "\n",
    "        ## Analysing the results \n",
    "        # Calculating sum of bandwidth, square of sum of bandwidth, count of flows and jfi and packet drop rate: \n",
    "\n",
    "        #To get packet dropped:\n",
    "        (drop_before,err_drop_before)=router_node.execute(\"tail --lines=10 tc_before\"+exp_name_str+\".txt| grep '\\\"drops\\\":' | awk '{print $2}' |cut -d ',' -f1\")\n",
    "\n",
    "        #To get packets sent\n",
    "        (sent_before,err_sent_before)=router_node.execute(\"tail --lines=10 tc_before\"+exp_name_str+\".txt| grep '\\\"packets\\\":' | awk '{print $2}' |cut -d ',' -f1\")\n",
    "\n",
    "        #To get packet dropped:\n",
    "        (drop_after,err_drop_after)=router_node.execute(\"tail --lines=10 tc_after\"+exp_name_str+\".txt| grep '\\\"drops\\\":' | awk '{print $2}' |cut -d ',' -f1\")\n",
    "\n",
    "        #To get packets sent\n",
    "        (sent_after, err_sent_after)=router_node.execute(\"tail --lines=10 tc_after\"+exp_name_str+\".txt| grep '\\\"packets\\\":' | awk '{print $2}' |cut -d ',' -f1\")\n",
    "\n",
    "        #Calculate packet drop rate:\n",
    "        n_seg_dropped=int(drop_after)-int(drop_before)\n",
    "        n_seg_sent=int(sent_after)-int(sent_before)\n",
    "        drop_rate=float(n_seg_dropped)/float(n_seg_sent)\n",
    "\n",
    "        print(\"Experiment name: \" + exp_name_str)\n",
    "        print(\"packet drop before running experiment: \"+ str(drop_before))\n",
    "        print(\"packet sent before running experiment: \" + str(sent_before))\n",
    "        print(\"packet drop after running experiment: \"+ str(drop_after))\n",
    "        print(\"packet sent after  running experiment: \" + str(sent_after))\n",
    "        print(\"packet sent: \" + str(n_seg_sent))\n",
    "        print(\"packet dropped: \" + str(n_seg_dropped))\n",
    "        print(\"packet drop rate: \" + str(drop_rate))\n",
    "\n",
    "        #Run the data processing scripts on each sender to get packet loss, congestion window halving events and rtt from iperf3 and ss output.\n",
    "        for i,n in enumerate(sender_nodes):\n",
    "            n.execute_thread(f'chmod +x process_cwn_file.py && python3 process_cwn_file.py '+str(i) )\n",
    "            n.execute_thread(f'chmod +x process_iperf_normal.py && python3 process_iperf_normal.py '+str(i)+\" \"+str(exp['num_servers'])+\" \"+str(exp['test_duration'])+\" \"+exp['cca']+\" \"+str(exp['flows']))\n",
    "\n",
    "        #Run mathis_sender.py script on each sender to get packet loss rate and cwnd halving rate of \n",
    "        #each flow and save the output to packet_loss_iperf{i}.csv file\n",
    "\n",
    "        while True:\n",
    "            i=False\n",
    "            time.sleep(100)\n",
    "            for n in sender_nodes:\n",
    "                (res,err)=n.execute(\"pgrep -af python\")\n",
    "                if \"python3 process_\" in res:\n",
    "                    i=True\n",
    "            if i:\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        for i,n in enumerate(sender_nodes):\n",
    "            n.execute('chmod +x mathis_sender.py && python3 mathis_sender.py '+str(i))\n",
    "\n",
    "        # Download all the packet_loss_iperf(i).csv file to the environment.\n",
    "        for i,n in enumerate(sender_nodes):\n",
    "            n.download_file(\"/home/fabric/work/IMC6/fabric-notebooks/packet_loss\"+str(i)+\".csv\", \"/home/ubuntu/packet_loss\"+str(i)+\".csv\")\n",
    "\n",
    "\n",
    "        ## Process the downloaded file to get the Mathis constant. Final output is saved to output_mathis_C.csv file.\n",
    "        dat_exp = pd.concat([pd.read_csv(\"packet_loss\"+str(i)+\".csv\") for i in range(n_endpoints) ], ignore_index=True)\n",
    "        dat_exp = dat_exp.assign(p_router_drop = n_seg_dropped/n_seg_sent )\n",
    "\n",
    "        coef_retrans_ss    = LinearRegression(fit_intercept = False).fit(\n",
    "            ( (1448*8*1000)/(dat_exp['rtt']*np.sqrt(dat_exp['p_ss_retrans'].values) ) ).values.reshape(-1,1), \n",
    "            dat_exp['bitrate']*1000.0\n",
    "        ).coef_\n",
    "        coef_retrans_iperf = LinearRegression(fit_intercept = False).fit(\n",
    "            ( (1448*8*1000)/(dat_exp['rtt']*np.sqrt(dat_exp['p_iperf_retrans'].values) ) ).values.reshape(-1,1), \n",
    "            dat_exp['bitrate']*1000.0\n",
    "        ).coef_\n",
    "        coef_cwnd_halve    = LinearRegression(fit_intercept = False).fit(\n",
    "            ( (1448*8*1000)/(dat_exp['rtt']*np.sqrt(dat_exp['p_cwnd_halve'].values) ) ).values.reshape(-1,1), \n",
    "            dat_exp['bitrate']*1000.0\n",
    "        ).coef_\n",
    "        coef_router_dropped = LinearRegression(fit_intercept = False).fit(\n",
    "            ( (1448*8*1000)/(dat_exp['rtt']*np.sqrt(dat_exp['p_router_drop'].values) ) ).values.reshape(-1,1), \n",
    "            dat_exp['bitrate']*1000.0\n",
    "        ).coef_\n",
    "        print(coef_retrans_ss, coef_retrans_iperf, coef_cwnd_halve, coef_router_dropped)\n",
    "\n",
    "        print( dat_exp.agg({'port': ['count'], 'bitrate': ['sum'], 'data_seg': ['sum'], 'retrans_ss': ['sum'], 'retrans_iperf': ['sum'], 'cwnd_halve': ['sum'], 'rtt': ['mean'] }) )\n",
    "\n",
    "        p=dat_exp['port'].aggregate('count')\n",
    "        bw=dat_exp['bitrate'].aggregate('sum')\n",
    "        seg=dat_exp['data_seg'].aggregate('sum')\n",
    "        retrans_ss_sum=dat_exp['retrans_ss'].aggregate('sum')\n",
    "        retrans_iperf_sum=dat_exp['retrans_iperf'].aggregate('sum')\n",
    "        cwn_halve_sum=dat_exp['cwnd_halve'].aggregate('sum')\n",
    "        rtt_mean=dat_exp['rtt'].aggregate('mean')\n",
    "\n",
    "\n",
    "        output_filename= data_dir + 'c_' + \"_\".join( [str(v) for v in exp.values()] )+\".csv\" # file with mathis constant\n",
    "        if not os.path.isfile(output_filename):\n",
    "            with open(output_filename, 'a', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                header = 'time_duration', 'ports', 'Base_RTT(ms)', 'BW', 'total_data_seg_out','total_cwnd_half', 'total_retransmission_ss',\\\n",
    "                    'total_retransmission_iperf', 'total_retransmission_ss/total_cwnd_half', 'total_retransmission_iperf/total_cwnd_half',\\\n",
    "                    'C_ss', 'C_iperf', 'C_cwnd', 'C_router', 'router_dropped', 'router_sent', 'router_dropped/total_cwnd_half', \\\n",
    "                    'mdape_ss', 'mdape_iperf', 'mdape_cwnd', 'mdape_router'\n",
    "                writer.writerow(header)\n",
    "\n",
    "        x_retrans_ss=( (1448*8*1000)/(dat_exp['rtt']*np.sqrt(dat_exp['p_ss_retrans'].values) ) ).values.reshape(-1,1)\n",
    "        x_retrans_iperf=( (1448*8*1000)/(dat_exp['rtt']*np.sqrt(dat_exp['p_iperf_retrans'].values) ) ).values.reshape(-1,1)\n",
    "        x_cwnd=( (1448*8*1000)/(dat_exp['rtt']*np.sqrt(dat_exp['p_cwnd_halve'].values) ) ).values.reshape(-1,1)\n",
    "        x_router=( (1448*8*1000)/(dat_exp['rtt']*np.sqrt(dat_exp['p_router_drop'].values) ) ).values.reshape(-1,1)\n",
    "\n",
    "        predicted_bw_ss    = LinearRegression(fit_intercept = False).fit(x_retrans_ss, dat_exp['bitrate']*1000.0).predict(x_retrans_ss)\n",
    "        predicted_bw_iperf = LinearRegression(fit_intercept = False).fit(x_retrans_iperf, dat_exp['bitrate']*1000.0).predict(x_retrans_iperf)\n",
    "        predicted_bw_cwnd    = LinearRegression(fit_intercept = False).fit(x_cwnd, dat_exp['bitrate']*1000.0).predict(x_cwnd)\n",
    "        predicted_bw_router = LinearRegression(fit_intercept = False).fit(x_router, dat_exp['bitrate']*1000.0).predict(x_router)\n",
    "\n",
    "        mdape_ss=np.median((np.abs(np.subtract(dat_exp['bitrate']*1000.0, predicted_bw_ss)/ (dat_exp['bitrate']*1000.0)))) * 100\n",
    "        mdape_iperf=np.median((np.abs(np.subtract(dat_exp['bitrate']*1000.0, predicted_bw_iperf)/ (dat_exp['bitrate']*1000.0)))) * 100\n",
    "        mdape_cwnd=np.median((np.abs(np.subtract(dat_exp['bitrate']*1000.0, predicted_bw_cwnd)/ (dat_exp['bitrate']*1000.0)))) * 100\n",
    "        mdape_router=np.median((np.abs(np.subtract(dat_exp['bitrate']*1000.0, predicted_bw_router)/ (dat_exp['bitrate']*1000.0)))) * 100\n",
    "\n",
    "        with open(output_filename, 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)   \n",
    "            columns = test_duration, p, delay, bw, seg, cwn_halve_sum, retrans_ss_sum,retrans_iperf_sum, retrans_ss_sum/cwn_halve_sum,\\\n",
    "                retrans_iperf_sum/cwn_halve_sum, coef_retrans_ss[0], coef_retrans_iperf[0], coef_cwnd_halve[0], coef_router_dropped[0],\\\n",
    "                n_seg_dropped, n_seg_sent, n_seg_dropped/cwn_halve_sum, mdape_ss, mdape_iperf,  mdape_cwnd, mdape_router\n",
    "            writer.writerow(columns)\n",
    "\n",
    "        #Get JFI and save it to file 'jfi.csv' \n",
    "        sq_y_values=(dat_exp['bitrate']*dat_exp['bitrate']).aggregate('sum')\n",
    "   \n",
    "        jfi_filename= data_dir + 'j_' + \"_\".join( [str(v) for v in exp.values()] )+\".csv\" # file with JFI\n",
    "        if not os.path.isfile(jfi_filename):\n",
    "            with open(jfi_filename, 'a', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                header ='CCA', 'Duration of Expt(sec)', 'Base RTT(ms)', 'Total Bandwidth(Kbps)', 'Sum of sq of BW', 'Flow Count', 'JFI'\n",
    "                writer.writerow(header)\n",
    "    \n",
    "        with open(jfi_filename, 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            columns = cca, test_duration, delay, bw , sq_y_values, p, (bw*bw)/(sq_y_values*p)\n",
    "            writer.writerow(columns)\n",
    "        \n",
    " \n",
    "        \n",
    "        # write a line to c_file_out\n",
    "        # write a line to j_file_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c951c",
   "metadata": {},
   "source": [
    "### Fig 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cae34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8b62d28",
   "metadata": {},
   "source": [
    "### Fig 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544b3615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "\n",
    "#dict_scale={'Edge':[0,50],'Intermediate':[90,500],'Core':[950,5000]}\n",
    "#dict_flows={'Edge':[10,30,50],'Intermediate':[100,300,500],'Core':[1000,3000,5000]}\n",
    "#rtt=[20,100,200]\n",
    "\n",
    "\n",
    "\n",
    "# List of filenames for core and edge \n",
    "c_files_core = [data_dir + 'c_' + \"_\".join( [str(v) for v in exp.values()] )+\".csv\" for exp in exp_lists_core]\n",
    "c_files_edge = [data_dir + 'c_' + \"_\".join( [str(v) for v in exp.values()] )+\".csv\" for exp in exp_lists_edge]\n",
    "\n",
    "#Read each CSV file and append its DataFrame to the list and concatenate it later\n",
    "# Core \n",
    "dfs = []\n",
    "for filename in c_files_core:\n",
    "   df = pd.read_csv(filename, header=0, \n",
    "                      names=['time_duration', 'ports', 'base_rtt', 'BW', 'total_data_seg_out','total_cwnd_half', 'total_retransmission_ss',\\\n",
    "        'total_retransmission_iperf', 'total_retransmission_ss_to_total_cwnd_half', 'total_retransmission_iperf_to_total_cwnd_half',\\\n",
    "        'C_ss', 'C_iperf', 'C_cwnd', 'C_router', 'router_dropped', 'router_sent', 'router_dropped_to_total_cwnd_half', \\\n",
    "        'mdape_ss', 'mdape_iperf', 'mdape_cwnd', 'mdape_router'])\n",
    "   dfs.append(df)  \n",
    "data_core = pd.concat(dfs, ignore_index=True) \n",
    "print(data_core)\n",
    "\n",
    "with PdfPages(\"Fig3_core_Ratio_plot.pdf\") as pdf:\n",
    "  #for r in rtt:\n",
    "    for key in dict_scale: \n",
    "        plt.figure()\n",
    "        plt.rcParams['figure.figsize'] = (5,2)\n",
    "        plt.rcParams['axes.axisbelow'] = True\n",
    "        plt.grid()\n",
    "        xvals = dat[(dict_scale[key][0] <= dat['ports']) & (dat['ports'] <= dict_scale[key][1]) & (dat['base_rtt']==r)]\n",
    "        print(xvals.router_droppedtototal_cwnd_half)\n",
    "        plt.plot(dict_flows[key],xvals.router_droppedtototal_cwnd_half)\n",
    "        plt.xlabel(\"Flow Count\")\n",
    "        plt.ylabel('Packet loss to CWND halving rate Ratio', wrap=True)\n",
    "        plt.title(key+\"Scale at base RTT of \"+str(r)+\"ms\")\n",
    "        plt.ylim(0,max(20,max(xvals.router_droppedtototal_cwnd_half)+2.5))\n",
    "        plt.yticks(np.arange(0,max(20,max(xvals.router_droppedtototal_cwnd_half)+2.5),5))\n",
    "        plt.xticks(dict_flows[key])\n",
    "        pdf.savefig(bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56551dcc-f9b7-4a74-b4e5-40052841705e",
   "metadata": {},
   "source": [
    "### Save the linear regression plots to a pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f842af0-b97b-4365-84b3-d74235381bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(\"linear_reg_plot.pdf\") as pdf:\n",
    "  plt.rcParams['figure.figsize'] = (8,6)\n",
    "\n",
    "  plt.scatter(x=x_retrans_ss, y=dat_exp['bitrate']*1000.0, color='C4', alpha=1, s=10, label='actual values')\n",
    "  plt.scatter(x=x_retrans_ss, y=predicted_bw_ss, color='C3',  alpha=1, s=10, label='predicted_values')\n",
    "  plt.plot(x_retrans_ss, predicted_bw_ss, color='C2', linewidth=0.5, label='fit')\n",
    "  plt.xlabel(\"x=mss/rtt*sqrt(packet_loss_rate)\") \n",
    "  plt.ylabel(\"y=bandwidth(bits/sec)\")\n",
    "  plt.title(\"Method-1 calculation of packet_loss rate using data_seg_out from ss and retrans from ss data\")\n",
    "  plt.legend()\n",
    "  pdf.savefig()  # saves the current figure into a pdf page\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "  plt.scatter(x=x_retrans_iperf, y=dat_exp['bitrate']*1000.0, color='C4', s=10, label='actual values')\n",
    "  plt.scatter(x=x_retrans_iperf, y=predicted_bw_iperf, color='C3', s=10, label='predicted_values')\n",
    "  plt.plot(x_retrans_iperf, predicted_bw_iperf, color='C2', linewidth=0.5, label='fit')\n",
    "  plt.xlabel(\"x=mss/rtt*sqrt(packet_loss_rate)\") \n",
    "  plt.ylabel(\"y=bandwidth(bits/sec)\")\n",
    "  plt.title(\"Method-2: calculation of packet_loss rate using data_seg_out from ss and retrans from iperf3 data\")\n",
    "  plt.legend()\n",
    "  pdf.savefig()  # saves the current figure into a pdf page\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "\n",
    "  plt.scatter(x=x_cwnd, y=dat_exp['bitrate']*1000.0, color='C4', alpha=1, s=10, label='actual values')\n",
    "  plt.scatter(x=x_cwnd, y=predicted_bw_cwnd, color='C3',  alpha=1, s=10, label='predicted_values')\n",
    "  plt.plot(x_cwnd, predicted_bw_cwnd, color='C2', linewidth=0.5, label='fit')\n",
    "  plt.xlabel(\"x=mss/rtt*sqrt(packet_loss_rate)\")\n",
    "  plt.ylabel(\"y=bandwidth(bits/sec)\")\n",
    "  plt.title(\"Method-3: calculation of packet_loss rate using data_seg_out from ss and cwnd from iperf3 data\")\n",
    "  plt.legend()\n",
    "  pdf.savefig()  # saves the current figure into a pdf page\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "  plt.scatter(x=x_router, y=dat_exp['bitrate']*1000.0, color='C4', alpha=1, s=10, label='actual values')\n",
    "  plt.scatter(x=x_router, y=predicted_bw_router, color='C3',  alpha=1, s=10, label='predicted_values')\n",
    "  plt.plot(x_router, predicted_bw_router, color='C2', linewidth=0.5, label='fit')\n",
    "  plt.xlabel(\"x=mss/rtt*sqrt(packet_loss_rate)\")\n",
    "  plt.ylabel(\"y=bandwidth(bits/sec)\")\n",
    "  plt.title(\"Method-4: calculation of packet_loss rate using packet drop rate at the router\")\n",
    "  plt.legend()\n",
    "  pdf.savefig()  # saves the current figure into a pdf page\n",
    "  plt.show()\n",
    "  plt.close()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7cd1c4-1af9-4c4d-9b35-df9184ac4f66",
   "metadata": {},
   "source": [
    "Remove the transfered files after data analysis and/or downloading to your local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fb9f05-d205-47a7-bc2f-ea4a1194c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm pac*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cced6741-06f4-470c-8f30-164214f5bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm linear*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
