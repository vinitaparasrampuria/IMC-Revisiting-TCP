{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02a5be90-977a-4948-a299-b5df71273eef",
   "metadata": {},
   "source": [
    "# Run experiments to \"Validate Mathis model at Edge, Core and Intermediate Scale\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d083f0-5028-4517-8d4e-eba13f28cac4",
   "metadata": {},
   "source": [
    "## Set up your FABRIC environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf58277-480d-4459-a9c6-08a085b1313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
    "fablib = fablib_manager() \n",
    "fablib.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a288b3d-4162-4249-8c9a-111ddb4beecf",
   "metadata": {},
   "source": [
    "## Get slice details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7704d48-a158-4c40-ba58-9ea4f462706c",
   "metadata": {},
   "source": [
    "Put your slice name and the number of endpoints in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3047eaf-4fb4-40c1-8dd0-c2f364685384",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_endpoints = 10\n",
    "slice_name=\"bottleneck-\" + str(n_endpoints) + '-test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e8f83-c35a-4669-98b5-647ef965efb1",
   "metadata": {},
   "source": [
    "Then, load your slice details into the environment.slice = fablib.new_slice(name=slice_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9af4c-28f9-4314-8791-878b4c41d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = fablib.get_slice(name=slice_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac3a090-f28c-4665-a470-06a9967decac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_nodes = [slice.get_node(name='sender-' + str(i))  for i in range(n_endpoints)]\n",
    "receiver_nodes = [slice.get_node(name='receiver-' + str(i))  for i in range(n_endpoints)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7db1cb-7e04-4077-bcd2-c135b78d8cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_node = slice.get_node(name='router')\n",
    "router_ingress_iface = router_node.get_interface(network_name = \"link-sender\")\n",
    "router_egress_iface  = router_node.get_interface(network_name = \"link-receiver\")\n",
    "router_egress_name = router_egress_iface.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43392b9f-43fc-4f4a-9734-f63bc99a882f",
   "metadata": {},
   "source": [
    "## Setup the scripts and modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb577e03-6a84-4edd-b340-6e6bc742a992",
   "metadata": {},
   "source": [
    "First, we will upload all the scripts we need to analyze the results for this experiment. This needs to be done only once before start of experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fcb13-840f-4ccd-b8b3-12d48bee1feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in sender_nodes:\n",
    "    n.upload_file('process_cwn_file.py','process_cwn_file.py')\n",
    "    n.upload_file('process_iperf_normal.py','process_iperf_normal.py')\n",
    "    n.upload_file('mathis_sender.py','mathis_sender.py')\n",
    "    n.upload_file(\"cwn.sh\", \"cwn.sh\")\n",
    "    n.upload_file('iperf-parallel-senders.sh','iperf-parallel-senders.sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cccfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in receiver_nodes:\n",
    "    n.upload_file('iperf-parallel-servers.sh','iperf-parallel-servers.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f66e6a-1ccb-4efb-b794-7f3f65ab925d",
   "metadata": {},
   "source": [
    "Also we need sklearn to process the downloaded files, so install the library in the environment.This needs to be done only once before start of experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3dcbb-afdc-464e-8e40-fac44e628586",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08443dd-e807-4b6d-9346-78d59c6352c8",
   "metadata": {},
   "source": [
    "## Generate flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a55ca7d-058b-4974-ba1e-47c074d9f50e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set experiment parameters\n",
    "\n",
    ">cca, delay, test_duration, num_servers, flows, interval, omit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b034aa90-b05f-4b1f-93d7-c7475100a6cb",
   "metadata": {},
   "source": [
    "cca is the congestion control algorithm (reno for this experiment)\n",
    "\n",
    "delay is the delay to be set at the receiver (20 ms,100 ms,200 ms)\n",
    "\n",
    "test_duration is the time for which to send the iperf3 flows (10800 is used for these experiments).\n",
    "\n",
    "num_servers is the number of ports to be opened on each receiver. \n",
    "CoreScale : 100,3000 and 500 ports\n",
    "EdgeScale : 1,3 and 5 ports\n",
    "IntermediateScale: 10, 30 and 50 ports\n",
    "\n",
    "flows is the number of parallel flows to be send from each port. It is set to 1 for all the experiments.\n",
    "\n",
    "interval is the periodic time interval to save the result by iperf3.\n",
    "\n",
    "omit is the starting n seconds to ignore the iperf values.(set to 0 for all the experiments. Max value=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba875fc8-39b8-4cd2-a677-6ec5eb2f5643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cca=\"reno\"\n",
    "# delay=200\n",
    "# test_duration=3600\n",
    "# num_servers=100\n",
    "# flows=1\n",
    "# interval=0.01\n",
    "# omit=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c690bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous\n",
    "import itertools\n",
    "exp_factors_core = {\n",
    "    'scenario': ['core'], \n",
    "    'rate': ['10Gbit'],\n",
    "    'limit': ['100MB'],\n",
    "    'cca': [\"reno\"],\n",
    "    'delay': [20],\n",
    "    'test_duration': [300],\n",
    "    'num_servers': [30, 50, 70],\n",
    "    'flows': [10],\n",
    "    'interval': [0.01],\n",
    "    'omit': [0],\n",
    "    'trial': [1]\n",
    "}\n",
    "factor_names = [k for k in exp_factors_core]\n",
    "factor_lists = list(itertools.product(*exp_factors_core.values()))\n",
    "exp_lists_core = [dict(zip(factor_names, factor_l)) for factor_l in factor_lists]\n",
    "\n",
    "exp_factors_edge = { \n",
    "    'scenario': ['edge'], \n",
    "    'rate': ['100Mbit'],\n",
    "    'limit': ['3MB'],\n",
    "    'cca': [\"reno\"],\n",
    "    'delay': [20],\n",
    "    'test_duration': [300],\n",
    "    'num_servers': [3, 5, 7],\n",
    "    'flows': [1],\n",
    "    'interval': [0.01],\n",
    "    'omit': [0],\n",
    "    'trial': [1]\n",
    "}\n",
    "factor_names = [k for k in exp_factors_edge]\n",
    "factor_lists = list(itertools.product(*exp_factors_edge.values()))\n",
    "exp_lists_edge = [dict(zip(factor_names, factor_l)) for factor_l in factor_lists]\n",
    "\n",
    "exp_lists = exp_lists_core + exp_lists_edge\n",
    "\n",
    "#exp_lists = exp_lists_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632032aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# check if directory exists, else create it\n",
    "directory_name = \"mathis-final-check\"\n",
    "if not os.path.exists(directory_name):\n",
    "    # Create the directory\n",
    "    os.mkdir(directory_name)\n",
    "    print(f\"Directory '{directory_name}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Directory '{directory_name}' already exists.\")\n",
    "current_working_directory = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c9d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_port = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d225edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # to allow resume\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.backends.backend_pdf\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import sys\n",
    "import os\n",
    "for exp in exp_lists:\n",
    "    \n",
    "    # check if we already ran this experiment\n",
    "    # (allow stop/resume)\n",
    "    data_dir = os.path.join(current_working_directory, directory_name)\n",
    "    exp_name_str =  \"_\".join( [str(v) for v in exp.values()] )\n",
    "    packet_loss_out = [data_dir + \"/packet_loss\"+ str(i) + exp_name_str+\".csv\" for i in range(n_endpoints)]\n",
    "\n",
    "    # check if the c file and j file already exist\n",
    "    if all(os.path.exists(file) for file in packet_loss_out):\n",
    "        print(\"Already have relevant files, skipping\")\n",
    "\n",
    "    else:\n",
    "        print(\"Running experiment to generate packet loss files for \", exp_name_str)#  \", \".join(packet_loss_out))\n",
    "\n",
    "        # set up edge or core scale setting\n",
    "        # first delete any existing queue\n",
    "        router_node.execute(\"sudo tc qdisc del dev \" + router_egress_name + \" root\")\n",
    "        # then set one up with HW offload\n",
    "        router_node.execute(\"sudo tc qdisc replace dev \" + router_egress_name + \" root handle 1: htb default 3 offload\")\n",
    "        router_node.execute(\"sudo tc class add dev \" + router_egress_name + \" parent 1: classid 1:3 htb rate \" + exp['rate'])\n",
    "        router_node.execute(\"sudo tc qdisc add dev \" + router_egress_name + \" parent 1:3 handle 3: bfifo limit \" + exp['limit'])\n",
    "\n",
    "        # TODO - move the experiment exec procedure inside the loop\n",
    "        ## Get queue statistics on the router before experiment\n",
    "        router_node.execute(\"tc -p -s -d -j qdisc show dev \"+router_egress_name +\" >tc_before.txt\")\n",
    "\n",
    "        ## Remove existing result files from the hosts #Check if the files are removed from the senders and receivers\n",
    "        for n in receiver_nodes:\n",
    "            n.execute(\"rm -f 60*\")\n",
    "        for n in sender_nodes:\n",
    "            n.execute(\"rm -f sender*\")              \n",
    "            n.execute(\"rm -f data*\")\n",
    "            n.execute(\"rm -f packet*\")\n",
    "            #n.execute(\"rm -f output*\")\n",
    "        print(\"printing before the experiment\")\n",
    "        #for n in sender_nodes:   \n",
    "        #    n.execute(\"ls\")\n",
    "\n",
    "        #Now set up delay on the receiver interface:\n",
    "        #First delete any existing queue (don't worry if there is an error, it means there was not!)\n",
    "        for n in receiver_nodes:\n",
    "            receiver_inf=n.get_interface(network_name= \"link-receiver\")\n",
    "            receiver_inf_name = receiver_inf.get_device_name()\n",
    "            n.execute(\"sudo tc qdisc del dev \" + receiver_inf_name + \" root netem\")\n",
    "            n.execute(\"sudo tc qdisc add dev \" + receiver_inf_name + \" root netem delay \" + str(exp['delay']) +\"ms limit 1000000\")\n",
    "        \n",
    "        ## Start parallel servers on the receiver          \n",
    "        for i, n in enumerate(receiver_nodes):\n",
    "            n.execute(\"sudo killall iperf3\")\n",
    "            n.execute_thread(f'chmod +x iperf-parallel-servers.sh && bash iperf-parallel-servers.sh '+str(exp['num_servers'])+\" \"+str(base_port))\n",
    "\n",
    "        #check if the required number of ports are opened\n",
    "        #for n in receiver_nodes:\n",
    "        #    n.execute(\"ls -1 | wc -l\")\n",
    "        \n",
    "\n",
    "        ## Start parallel clients on the sender\n",
    "        for i, n in enumerate(sender_nodes):\n",
    "            n.execute(\"sudo killall iperf3\")\n",
    "            n.execute_thread(f'chmod +x iperf-parallel-senders.sh && bash iperf-parallel-senders.sh 10.10.2.1'+str(i)+\" \"+str(exp['num_servers'])+\" \"+str(exp['test_duration'])+\" \"+exp['cca']+\" \"+str(exp['flows'])+\" \"+str(exp['interval'])+\" \"+str(exp['omit'])+\" \"+str(base_port))\n",
    "            n.execute_thread(f'chmod +x cwn.sh && bash cwn.sh 10.10.2.1'+str(i))\n",
    "        \n",
    "        #time.sleep(min(2*exp['test_duration'], exp['test_duration']+300))\n",
    "        time.sleep(exp['test_duration']+600)\n",
    "\n",
    "        ## Get queue statistics on the router after experiment\n",
    "        router_node.execute(\"tc -p -s -d -j qdisc show dev \"+router_egress_name +\" >tc_after\"+\".txt\")\n",
    "\n",
    "        ## Analysing the results \n",
    "        # Calculating sum of bandwidth, square of sum of bandwidth, count of flows and jfi and packet drop rate: \n",
    "\n",
    "        #To get packet dropped:\n",
    "        (drop_before,err_drop_before)=router_node.execute(\"tail --lines=10 tc_before.txt| grep '\\\"drops\\\":' | awk '{print $2}' |cut -d ',' -f1\")\n",
    "\n",
    "        #To get packets sent\n",
    "        (sent_before,err_sent_before)=router_node.execute(\"tail --lines=10 tc_before.txt| grep '\\\"packets\\\":' | awk '{print $2}' |cut -d ',' -f1\")\n",
    "\n",
    "        #To get packet dropped:\n",
    "        (drop_after,err_drop_after)=router_node.execute(\"tail --lines=10 tc_after.txt| grep '\\\"drops\\\":' | awk '{print $2}' |cut -d ',' -f1\")\n",
    "\n",
    "        #To get packets sent\n",
    "        (sent_after, err_sent_after)=router_node.execute(\"tail --lines=10 tc_after.txt| grep '\\\"packets\\\":' | awk '{print $2}' |cut -d ',' -f1\")\n",
    "        \n",
    "        #Calculate packet drop rate:\n",
    "        #n_seg_dropped=int(drop_after.replace('\\n', ''))-int(drop_before.replace('\\n', ''))\n",
    "        #n_seg_sent=int(sent_after.replace('\\n', ''))-int(sent_before.replace('\\n', ''))\n",
    "        n_seg_dropped=int(drop_after)-int(drop_before)\n",
    "        n_seg_sent=int(sent_after)-int(sent_before)\n",
    "        drop_rate=int(n_seg_dropped)/int(n_seg_sent)\n",
    "\n",
    "        #print(\"Experiment name: \" + exp_name_str)\n",
    "        #print(\"packet drop before running experiment: \"+ str(drop_before.replace('\\n', '')))\n",
    "        #print(\"packet sent before running experiment: \" + str(sent_before.replace('\\n', '')))\n",
    "        #print(\"packet drop after running experiment: \"+ str(drop_after.replace('\\n', '')))\n",
    "        #print(\"packet sent after  running experiment: \" + str(sent_after.replace('\\n', '')))\n",
    "        #print(\"packet sent: \" + str(n_seg_sent))\n",
    "        #print(\"packet dropped: \" + str(n_seg_dropped))\n",
    "        #print(\"packet drop rate: \" + str(drop_rate))\n",
    "        \n",
    "        #Run the data processing scripts on each sender to get packet loss, congestion window halving events and rtt from iperf3 and ss output.\n",
    "        for i,n in enumerate(sender_nodes):\n",
    "            if (i==n_endpoints-1):\n",
    "                n.execute(f'chmod +x process_cwn_file.py && python3 process_cwn_file.py '+str(i) )\n",
    "            else: \n",
    "                n.execute_thread(f'chmod +x process_cwn_file.py && python3 process_cwn_file.py '+str(i) )\n",
    "        for i,n in enumerate(sender_nodes):\n",
    "            if (i==n_endpoints-1):\n",
    "                n.execute(f'chmod +x process_iperf_normal.py && python3 process_iperf_normal.py '+str(i)+\" \"+str(exp['num_servers'])+\" \"+str(exp['test_duration'])+\" \"+exp['cca']+\" \"+str(exp['flows']))\n",
    "            else: \n",
    "                n.execute_thread(f'chmod +x process_iperf_normal.py && python3 process_iperf_normal.py '+str(i)+\" \"+str(exp['num_servers'])+\" \"+str(exp['test_duration'])+\" \"+exp['cca']+\" \"+str(exp['flows']))\n",
    " \n",
    "        while True:\n",
    "            i=False\n",
    "            time.sleep(100)\n",
    "            for n in sender_nodes:\n",
    "                (res,err)=n.execute(\"pgrep -af python\")\n",
    "                print(\"res\", res)\n",
    "                if \"python3 process_\" in res:\n",
    "                    i=True\n",
    "            if i:\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        for i,n in enumerate(sender_nodes):\n",
    "            n.execute('chmod +x mathis_sender.py && python3 mathis_sender.py '+str(i)+ \" \" + str(n_seg_sent)+ \" \" + str(n_seg_dropped))\n",
    "\n",
    "        # Download all the packet_loss_iperf(i).csv file to the environment.\n",
    "        for i,n in enumerate(sender_nodes):\n",
    "            n.download_file(data_dir + \"/packet_loss\"+ str(i) + exp_name_str+\".csv\", \"/home/ubuntu/packet_loss\"+str(i)+\".csv\")\n",
    "            \n",
    "        for i,n in enumerate(sender_nodes):\n",
    "            dat_exp = pd.concat([pd.read_csv(data_dir + \"/packet_loss\"+str(i)+ exp_name_str+\".csv\") for i in range(n_endpoints) ], ignore_index=True)\n",
    "            print(dat_exp['port'].aggregate('count'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67af1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data analysis \n",
    "#Get JFI and save it to file 'jfi.csv'                      \n",
    "# write a line to c_file_out\n",
    "# write a line to j_file_out\n",
    "\n",
    "directory_name = \"mathis-final-check\"\n",
    "data_dir = os.path.join(current_working_directory, directory_name)\n",
    "for exp in exp_lists:\n",
    "    exp_name_str =  \"_\".join( [str(v) for v in exp.values()] )    \n",
    "    c_file_out = data_dir + '/c_' + \"_\".join( [str(v) for v in exp.values()] )+\".csv\" # file with mathis constant\n",
    "    j_file_out = data_dir + '/j_' + \"_\".join( [str(v) for v in exp.values()] )+\".csv\" # file with JFI\n",
    "    \n",
    "    if (os.path.exists(c_file_out)) and (os.path.exists(j_file_out)):\n",
    "        print(\"Already have \" + c_file_out + \" and \" + j_file_out + \", skipping\")\n",
    "    else:\n",
    "        #print(\"Running experiment to generate \" +  c_file_out + \" and \" + j_file_out )\n",
    "        ## Process the downloaded file to get the Mathis constant. Final output is saved to output_mathis_C.csv file.\n",
    "        dat_exp = []\n",
    "        dat_exp = pd.concat([pd.read_csv(data_dir + \"/packet_loss\"+str(i)+ exp_name_str+\".csv\") for i in range(n_endpoints) ], ignore_index=True)\n",
    "        print(dat_exp['port'].aggregate('count'))\n",
    "        coef_retrans_ss    = LinearRegression(fit_intercept = False).fit(\n",
    "            ( (1448*8*1000)/(dat_exp['rtt']*np.sqrt(dat_exp['p_ss_retrans'].values) ) ).values.reshape(-1,1), \n",
    "            dat_exp['bitrate']*1000.0\n",
    "        ).coef_\n",
    "        coef_retrans_iperf = LinearRegression(fit_intercept = False).fit(\n",
    "            ( (1448*8*1000)/(dat_exp['rtt']*np.sqrt(dat_exp['p_iperf_retrans'].values) ) ).values.reshape(-1,1), \n",
    "            dat_exp['bitrate']*1000.0\n",
    "        ).coef_\n",
    "        coef_cwnd_halve    = LinearRegression(fit_intercept = False).fit(\n",
    "            ( (1448*8*1000)/(dat_exp['rtt']*np.sqrt(dat_exp['p_cwnd_halve'].values) ) ).values.reshape(-1,1), \n",
    "            dat_exp['bitrate']*1000.0\n",
    "        ).coef_\n",
    "        coef_router_dropped = LinearRegression(fit_intercept = False).fit(\n",
    "            ( (1448*8*1000)/(dat_exp['rtt']*np.sqrt(dat_exp['p_router_drop'].values) ) ).values.reshape(-1,1), \n",
    "            dat_exp['bitrate']*1000.0\n",
    "        ).coef_\n",
    "        #print(coef_retrans_ss, coef_retrans_iperf, coef_cwnd_halve, coef_router_dropped)\n",
    "\n",
    "        #print( dat_exp.agg({'port': ['count'], 'bitrate': ['sum'], 'data_seg': ['sum'], 'retrans_ss': ['sum'], 'retrans_iperf': ['sum'], 'cwnd_halve': ['sum'], 'rtt': ['mean'] }) )\n",
    "\n",
    "        p=dat_exp['port'].aggregate('count')\n",
    "        print(\"Ports\", p)\n",
    "        bw=dat_exp['bitrate'].aggregate('sum')\n",
    "        seg=dat_exp['data_seg'].aggregate('sum')\n",
    "        retrans_ss_sum=dat_exp['retrans_ss'].aggregate('sum')\n",
    "        retrans_iperf_sum=dat_exp['retrans_iperf'].aggregate('sum')\n",
    "        cwn_halve_sum=dat_exp['cwnd_halve'].aggregate('sum')\n",
    "        rtt_mean=dat_exp['rtt'].aggregate('mean')\n",
    "        n_seg_dropped = dat_exp['p_n_seg_dropped'].aggregate('mean')\n",
    "        n_seg_sent = dat_exp['p_n_seg_sent'].aggregate('mean')\n",
    "        \n",
    "\n",
    "        output_filename= data_dir + '/c_' + \"_\".join( [str(v) for v in exp.values()] )+\".csv\" # file with mathis constant\n",
    "        if not os.path.isfile(output_filename):\n",
    "            with open(output_filename, 'a', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                header = 'time_duration', 'ports', 'Base_RTT(ms)', 'BW', 'total_data_seg_out','total_cwnd_half', 'total_retransmission_ss',\\\n",
    "                    'total_retransmission_iperf', 'total_retransmission_ss/total_cwnd_half', 'total_retransmission_iperf/total_cwnd_half',\\\n",
    "                    'C_ss', 'C_iperf', 'C_cwnd', 'C_router', 'router_dropped', 'router_sent', 'router_dropped/total_cwnd_half', \\\n",
    "                    'mdape_ss', 'mdape_iperf', 'mdape_cwnd', 'mdape_router'\n",
    "                writer.writerow(header)\n",
    "\n",
    "        x_retrans_ss=( (1448*8*1000)/(dat_exp['rtt']*np.sqrt(dat_exp['p_ss_retrans'].values) ) ).values.reshape(-1,1)\n",
    "        x_retrans_iperf=( (1448*8*1000)/(dat_exp['rtt']*np.sqrt(dat_exp['p_iperf_retrans'].values) ) ).values.reshape(-1,1)\n",
    "        x_cwnd=( (1448*8*1000)/(dat_exp['rtt']*np.sqrt(dat_exp['p_cwnd_halve'].values) ) ).values.reshape(-1,1)\n",
    "        x_router=( (1448*8*1000)/(dat_exp['rtt']*np.sqrt(dat_exp['p_router_drop'].values) ) ).values.reshape(-1,1)\n",
    "\n",
    "        predicted_bw_ss    = LinearRegression(fit_intercept = False).fit(x_retrans_ss, dat_exp['bitrate']*1000.0).predict(x_retrans_ss)\n",
    "        predicted_bw_iperf = LinearRegression(fit_intercept = False).fit(x_retrans_iperf, dat_exp['bitrate']*1000.0).predict(x_retrans_iperf)\n",
    "        predicted_bw_cwnd    = LinearRegression(fit_intercept = False).fit(x_cwnd, dat_exp['bitrate']*1000.0).predict(x_cwnd)\n",
    "        predicted_bw_router = LinearRegression(fit_intercept = False).fit(x_router, dat_exp['bitrate']*1000.0).predict(x_router)\n",
    "\n",
    "        mdape_ss=np.median((np.abs(np.subtract(dat_exp['bitrate']*1000.0, predicted_bw_ss)/ (dat_exp['bitrate']*1000.0)))) * 100\n",
    "        mdape_iperf=np.median((np.abs(np.subtract(dat_exp['bitrate']*1000.0, predicted_bw_iperf)/ (dat_exp['bitrate']*1000.0)))) * 100\n",
    "        mdape_cwnd=np.median((np.abs(np.subtract(dat_exp['bitrate']*1000.0, predicted_bw_cwnd)/ (dat_exp['bitrate']*1000.0)))) * 100\n",
    "        mdape_router=np.median((np.abs(np.subtract(dat_exp['bitrate']*1000.0, predicted_bw_router)/ (dat_exp['bitrate']*1000.0)))) * 100\n",
    "\n",
    "        with open(output_filename, 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)   \n",
    "            columns = exp['test_duration'], p, exp['delay'], bw, seg, cwn_halve_sum, retrans_ss_sum,retrans_iperf_sum, retrans_ss_sum/cwn_halve_sum,\\\n",
    "                retrans_iperf_sum/cwn_halve_sum, coef_retrans_ss[0], coef_retrans_iperf[0], coef_cwnd_halve[0], coef_router_dropped[0],\\\n",
    "                n_seg_dropped, n_seg_sent, n_seg_dropped/cwn_halve_sum, mdape_ss, mdape_iperf,  mdape_cwnd, mdape_router\n",
    "            writer.writerow(columns)\n",
    "\n",
    "        \n",
    "        sq_y_values=(dat_exp['bitrate']*dat_exp['bitrate']).aggregate('sum')\n",
    "   \n",
    "        jfi_filename= data_dir + '/j_' + \"_\".join( [str(v) for v in exp.values()] )+\".csv\" # file with JFI\n",
    "        if not os.path.isfile(jfi_filename):\n",
    "            with open(jfi_filename, 'a', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                header ='CCA', 'Duration of Expt(sec)', 'Base RTT(ms)', 'Total Bandwidth(Kbps)', 'Sum of sq of BW', 'Flow Count', 'JFI'\n",
    "                writer.writerow(header)\n",
    "    \n",
    "        with open(jfi_filename, 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            columns = exp['cca'], exp['test_duration'], exp['delay'], bw , sq_y_values, p, (bw*bw)/(sq_y_values*p)\n",
    "            writer.writerow(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5946cdd9",
   "metadata": {},
   "source": [
    "### Plots "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a316a6ad",
   "metadata": {},
   "source": [
    "Installing required packages required for the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a3d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "!pip install pdfkit\n",
    "!pip install reportlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635fd0ad",
   "metadata": {},
   "source": [
    "### Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079eeec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib import colors\n",
    "from reportlab.platypus import Table, TableStyle, SimpleDocTemplate\n",
    "import pandas as pd\n",
    "\n",
    "directory_name = \"mathis-final-check\"\n",
    "new_directory_name = \"mathis-final-check-plots\"\n",
    "#directory_name = \"data\"\n",
    "current_working_directory = os.getcwd()\n",
    "data_dir = os.path.join(current_working_directory, directory_name)\n",
    "plots_dir = os.path.join(current_working_directory, new_directory_name)\n",
    "\n",
    "if not os.path.exists(new_directory_name):\n",
    "    os.mkdir(new_directory_name)\n",
    "    print(f\"Directory '{new_directory_name}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Directory '{new_directory_name}' already exists.\")\n",
    "\n",
    "\n",
    "#change the filename, default filename in which the data is stored is \"output_mathis_C.csv\"\n",
    "c_files_core = [data_dir + '/c_' + \"_\".join( [str(v) for v in exp.values()] )+\".csv\" for exp in exp_lists_core]\n",
    "c_files_edge = [data_dir + '/c_' + \"_\".join( [str(v) for v in exp.values()] )+\".csv\" for exp in exp_lists_edge]\n",
    "\n",
    "#Read each CSV file and append its DataFrame to the list and concatenate it later\n",
    "# Core \n",
    "dfs = []\n",
    "for filename in c_files_core:\n",
    "    df = pd.read_csv(filename, header=0,\n",
    "                      names=['time_duration', 'ports', 'base_rtt', 'BW', 'total_data_seg_out','total_cwnd_half', 'total_retransmission_ss',\\\n",
    "        'total_retransmission_iperf', 'total_retransmission_ss_to_total_cwnd_half', 'total_retransmission_iperf_to_total_cwnd_half',\\\n",
    "        'C_ss', 'C_iperf', 'C_cwnd', 'C_router', 'router_dropped', 'router_sent', 'router_dropped_to_total_cwnd_half', \\\n",
    "        'mdape_ss', 'mdape_iperf', 'mdape_cwnd', 'mdape_router'])\n",
    "    dfs.append(df)  \n",
    "data_core = pd.concat(dfs, ignore_index=True) \n",
    "#print(data_core)\n",
    "#data_core['ports'] = [1000,3000,5000]\n",
    "\n",
    "dfs = []\n",
    "for filename in c_files_edge:\n",
    "    df = pd.read_csv(filename, header=0,\n",
    "                      names=['time_duration', 'ports', 'base_rtt', 'BW', 'total_data_seg_out','total_cwnd_half', 'total_retransmission_ss',\\\n",
    "        'total_retransmission_iperf', 'total_retransmission_ss_to_total_cwnd_half', 'total_retransmission_iperf_to_total_cwnd_half',\\\n",
    "        'C_ss', 'C_iperf', 'C_cwnd', 'C_router', 'router_dropped', 'router_sent', 'router_dropped_to_total_cwnd_half', \\\n",
    "        'mdape_ss', 'mdape_iperf', 'mdape_cwnd', 'mdape_router'])\n",
    "    dfs.append(df)  \n",
    "data_edge = pd.concat(dfs, ignore_index=True) \n",
    "\n",
    "xvals_core=data_core.sort_values(by=['ports'])\n",
    "xvals_edge = data_edge\n",
    "packet_loss_C= [xvals_edge['C_router'].aggregate('mean')]\n",
    "packet_loss=packet_loss_C+xvals_core['C_router'].values.tolist()\n",
    "flow_num = pd.unique(xvals_core.ports).tolist()\n",
    "\n",
    "#print(flow_num)\n",
    "#print(packet_loss)\n",
    "CWND_half_C= [xvals_edge['C_cwnd'].aggregate('mean')]\n",
    "data = { \n",
    "    \"Packet Loss\": packet_loss_C+xvals_core['C_router'].values.tolist(),\n",
    "    \"CWND Halving\": CWND_half_C+xvals_core['C_cwnd'].values.tolist()\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "cols = df.columns.tolist()\n",
    "for prsn in range(0, df.shape[1]):\n",
    "    df[cols[prsn]] = df[cols[prsn]].apply(lambda x: f'{\"{:.2f}\".format(float(x))}')\n",
    "df = df.T.reset_index()\n",
    "my_list = [[\"p(20ms)\", 'EdgeScale', \"Core Scale Flow Count\"]]+ [[\"Flow\",\" \"]+ flow_num] + df.values.tolist()\n",
    "table = Table(my_list)\n",
    "table.setStyle(TableStyle([('SPAN', (0, 0), (0, 1)),\n",
    "                           (('SPAN', (1, 0), (1, 1))),\n",
    "                  ('SPAN', (2, 0), (-1, 0)),\n",
    "                  ('BACKGROUND', (0,0), (-1,0), colors.white),\n",
    "                  ('BACKGROUND',(0,1),(-1,-1),colors.white),\n",
    "                  ('BACKGROUND', (0,0), (0,1), colors.white),\n",
    "                  ('TEXTCOLOR',(0,0),(-1,0),colors.black),\n",
    "                  ('ALIGN', (0,0), (-1,-1), 'CENTER'),\n",
    "                  ('FONTNAME', (0,0), (-1,0), 'Helvetica'),\n",
    "                  ('FONTSIZE', (0,0), (-1,0), 12),\n",
    "                  ('BOTTOMPADDING', (0,0), (-1,0), 12),\n",
    "                  ('LINEABOVE', (1, 2), (-1, 2), 1, colors.black),\n",
    "                  ('GRID',(0, 0), (-1,-1),1,colors.black)]))\n",
    "\n",
    "pdf_file = plots_dir + '/Table1_core.pdf'\n",
    "c = canvas.Canvas(pdf_file, pagesize=letter)\n",
    "table.wrapOn(c, inch*7, inch*2)\n",
    "table.drawOn(c, x=50, y=650)\n",
    "#c.showPage()\n",
    "c.save()\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c951c",
   "metadata": {},
   "source": [
    "### Fig 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cae34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "#dict_scale={'Edge':[0,50],'Intermediate':[90,500],'Core':[950,5000]}\n",
    "#dict_flows={'Edge':[10,30,50],'Intermediate':[100,300,500],'Core':[1000,3000,5000]}\n",
    "#rtt=[20,100,200]\n",
    "\n",
    "directory_name = \"mathis-final-check\"\n",
    "new_directory_name = \"mathis-final-check-plots\"\n",
    "current_working_directory = os.getcwd()\n",
    "data_dir = os.path.join(current_working_directory, directory_name)\n",
    "plots_dir = os.path.join(current_working_directory, new_directory_name)\n",
    "\n",
    "#change the filename, default filename in which the data is stored is \"output_mathis_C.csv\"\n",
    "c_files_core = [data_dir + '/c_' + \"_\".join( [str(v) for v in exp.values()] )+\".csv\" for exp in exp_lists_core]\n",
    "c_files_edge = [data_dir + '/c_' + \"_\".join( [str(v) for v in exp.values()] )+\".csv\" for exp in exp_lists_edge]\n",
    "\n",
    "#c_files_core = [\"output_mathis_C.csv\" for exp in exp_lists_core]\n",
    "#c_files_edge = [\"output_mathis_C.csv\" for exp in exp_lists_core]\n",
    "\n",
    "#Read each CSV file and append its DataFrame to the list and concatenate it later\n",
    "# Core \n",
    "dfs = []\n",
    "for filename in c_files_core:\n",
    "    df = pd.read_csv(filename, header=0,\n",
    "                      names=['time_duration', 'ports', 'base_rtt', 'BW', 'total_data_seg_out','total_cwnd_half', 'total_retransmission_ss',\\\n",
    "        'total_retransmission_iperf', 'total_retransmission_ss_to_total_cwnd_half', 'total_retransmission_iperf_to_total_cwnd_half',\\\n",
    "        'C_ss', 'C_iperf', 'C_cwnd', 'C_router', 'router_dropped', 'router_sent', 'router_dropped_to_total_cwnd_half', \\\n",
    "        'mdape_ss', 'mdape_iperf', 'mdape_cwnd', 'mdape_router'])\n",
    "    dfs.append(df)  \n",
    "data_core = pd.concat(dfs, ignore_index=True) \n",
    "#print(data_core)\n",
    "\n",
    "# Edge\n",
    "dfs = []\n",
    "for filename in c_files_edge:\n",
    "    df = pd.read_csv(filename, header=0,\n",
    "                      names=['time_duration', 'ports', 'base_rtt', 'BW', 'total_data_seg_out','total_cwnd_half', 'total_retransmission_ss',\\\n",
    "        'total_retransmission_iperf', 'total_retransmission_ss_to_total_cwnd_half', 'total_retransmission_iperf_to_total_cwnd_half',\\\n",
    "        'C_ss', 'C_iperf', 'C_cwnd', 'C_router', 'router_dropped', 'router_sent', 'router_dropped_to_total_cwnd_half', \\\n",
    "        'mdape_ss', 'mdape_iperf', 'mdape_cwnd', 'mdape_router'])\n",
    "    dfs.append(df)  \n",
    "data_edge = pd.concat(dfs, ignore_index=True) \n",
    "\n",
    "\n",
    "N = 3\n",
    "ind = np.arange(N)\n",
    "width = 0.25\n",
    "with PdfPages(plots_dir + \"/MedianError_plot.pdf\") as pdf:\n",
    "    plt.rcParams['figure.figsize'] = (5,2)\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.set_axisbelow(True)\n",
    "    ax1.grid()\n",
    "    xvals=data_core.sort_values(by=['ports'])\n",
    "    bar1 = ax1.bar(ind, xvals.mdape_router, width, color = 'chocolate')\n",
    "    bar2 = ax1.bar(ind+width+0.02, xvals.mdape_cwnd, width, color = 'blue')\n",
    "    port_num_core=pd.unique(xvals.ports)\n",
    "    \n",
    "    ax1.set_xlabel(\"Flow Count\")\n",
    "    ax1.set_ylabel('Error (%)')\n",
    "    ax1.set_title(\"Core Scale at Base RTT of 20ms\")\n",
    "    ax1.set_xticks(ind+0.1, labels=port_num_core)\n",
    "    ax1.legend( (bar1,bar2), (\"Packet Loss Rate\", \"CWND Halving Rate\"), loc='upper left', bbox_to_anchor=(1, 1), frameon=False )\n",
    "    ax1.set_ylim(0,20)\n",
    "\n",
    "    xvals_edge = data_edge.sort_values(by=['ports'])\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.tick_params(top=False, right=False, labelright=False, labeltop=False, gridOn=False)\n",
    "    ax3 = ax2.twinx()\n",
    "    port_num_edge=pd.unique(xvals_edge.ports)\n",
    "    #print(port_num_edge)\n",
    "    \n",
    "    ax3.plot(port_num_edge,xvals_edge.mdape_router, color='red')\n",
    "    ax3.set_ylim(0,20)\n",
    "    ax3.legend( [\"Home, Packet Loss\"], bbox_to_anchor=(1, 0.3),  loc='upper left', frameon=False)\n",
    "    ax3.tick_params(top=False, right=False, labelright=False, labeltop=False, gridOn=False)\n",
    "    ax4 = ax2.twinx()\n",
    "    ax4.plot(port_num_edge,xvals_edge.mdape_cwnd, color='cyan')\n",
    "    ax4.set_ylim(0,20)\n",
    "    ax4.legend( [\"Home, CWND Halving\"], bbox_to_anchor=(1, 0.2),  loc='upper left', frameon=False)\n",
    "    ax4.tick_params(top=False, right=False, labelright=False, labeltop=False, gridOn=False)\n",
    "    pdf.savefig(bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b62d28",
   "metadata": {},
   "source": [
    "### Fig 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544b3615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "\n",
    "directory_name = \"mathis-final-check\"\n",
    "new_directory_name = \"mathis-final-check-plots\"\n",
    "current_working_directory = os.getcwd()\n",
    "data_dir = os.path.join(current_working_directory, directory_name)\n",
    "plots_dir = os.path.join(current_working_directory, new_directory_name)\n",
    "\n",
    "\n",
    "# List of filenames for core and edge \n",
    "c_files_core = [data_dir + '/c_' + \"_\".join( [str(v) for v in exp.values()] )+\".csv\" for exp in exp_lists_core]\n",
    "c_files_edge = [data_dir + '/c_' + \"_\".join( [str(v) for v in exp.values()] )+\".csv\" for exp in exp_lists_edge]\n",
    "\n",
    "\n",
    "#Read each CSV file and append its DataFrame to the list and concatenate it later\n",
    "# First we will do the Core \n",
    "dfs = []\n",
    "for filename in c_files_core:\n",
    "    df = pd.read_csv(filename, header=0, \n",
    "                      names=['time_duration', 'ports', 'base_rtt', 'BW', 'total_data_seg_out','total_cwnd_half', 'total_retransmission_ss',\\\n",
    "        'total_retransmission_iperf', 'total_retransmission_ss_to_total_cwnd_half', 'total_retransmission_iperf_to_total_cwnd_half',\\\n",
    "        'C_ss', 'C_iperf', 'C_cwnd', 'C_router', 'router_dropped', 'router_sent', 'router_dropped_to_total_cwnd_half', \\\n",
    "        'mdape_ss', 'mdape_iperf', 'mdape_cwnd', 'mdape_router'])\n",
    "    dfs.append(df)  \n",
    "data_core = pd.concat(dfs, ignore_index=True) \n",
    "#print(data_core)\n",
    "\n",
    "#core_flows = [3000, 5000, 7000]\n",
    "#edge_flows = [3, 5, 7]\n",
    "\n",
    "with PdfPages(plots_dir + \"/Fig3_core_Ratio_plot.pdf\") as pdf:\n",
    "        plt.figure()\n",
    "        plt.rcParams['figure.figsize'] = (5,2)\n",
    "        plt.rcParams['axes.axisbelow'] = True\n",
    "        plt.grid()\n",
    "        #xvals=data_core.sort_values(by=['ports'])\n",
    "        Y_val = data_core['router_dropped_to_total_cwnd_half'].tolist()\n",
    "        X_val = data_core['ports'].tolist()\n",
    "        print(X_val)\n",
    "        plt.plot(X_val,Y_val)\n",
    "    \n",
    "        plt.xlabel(\"Flow Count\")\n",
    "        plt.ylabel('Packet loss to CWND halving rate Ratio(%)', wrap=True)\n",
    "        plt.title(\"Core Scale at base RTT of 20ms\")\n",
    "        plt.ylim(0,max(20,max(Y_val)+2.5))\n",
    "        plt.yticks(np.arange(0,max(20,max(Y_val)+2.5),5))\n",
    "        plt.xticks(X_val)\n",
    "        pdf.savefig(bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        \n",
    "# Now we do for the edge \n",
    "dfs = []\n",
    "for filename in c_files_edge:\n",
    "    df = pd.read_csv(filename, header=0, \n",
    "                      names=['time_duration', 'ports', 'base_rtt', 'BW', 'total_data_seg_out','total_cwnd_half', 'total_retransmission_ss',\\\n",
    "        'total_retransmission_iperf', 'total_retransmission_ss_to_total_cwnd_half', 'total_retransmission_iperf_to_total_cwnd_half',\\\n",
    "        'C_ss', 'C_iperf', 'C_cwnd', 'C_router', 'router_dropped', 'router_sent', 'router_dropped_to_total_cwnd_half', \\\n",
    "        'mdape_ss', 'mdape_iperf', 'mdape_cwnd', 'mdape_router'])\n",
    "    dfs.append(df)  \n",
    "data_edge = pd.concat(dfs, ignore_index=True) \n",
    "#print(data_core)\n",
    "\n",
    "with PdfPages(plots_dir + \"/Fig3_edge_Ratio_plot.pdf\") as pdf:\n",
    "        plt.figure()\n",
    "        plt.rcParams['figure.figsize'] = (5,2)\n",
    "        plt.rcParams['axes.axisbelow'] = True\n",
    "        plt.grid()\n",
    "        Y_val = data_edge['router_dropped_to_total_cwnd_half'].tolist()\n",
    "        X_val = data_edge['ports'].tolist()      \n",
    "        plt.plot(X_val,Y_val)\n",
    "        plt.xlabel(\"Flow Count\")\n",
    "        plt.ylabel('Packet loss to CWND halving rate Ratio(%)', wrap=True)\n",
    "        plt.title(\"Edge Scale at base RTT of 20ms\")\n",
    "        plt.ylim(0,max(20,max(Y_val)+2.5))\n",
    "        plt.yticks(np.arange(0,max(20,max(Y_val)+2.5),5))\n",
    "        plt.xticks(X_val)\n",
    "        pdf.savefig(bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4dc4e1",
   "metadata": {},
   "source": [
    "### Fig 4 : Not here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a42fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "\n",
    "# N = 3\n",
    "# ind = np.arange(N) \n",
    "# width = 0.25\n",
    "# color_name = {'20':'royalblue', '100':'tomato', '200':'mediumspringgreen'}\n",
    "# xvals = [0,0,0]\n",
    "# bar = [0,0,0]\n",
    "\n",
    "# directory_name = \"mathis-final-check\"\n",
    "# new_directory_name = \"mathis-final-check-plots\"\n",
    "# current_working_directory = os.getcwd()\n",
    "# data_dir = os.path.join(current_working_directory, directory_name)\n",
    "# plots_dir = os.path.join(current_working_directory, new_directory_name)\n",
    "\n",
    "# # List of filenames for core and edge \n",
    "# jfi_files_core = [data_dir + '/j_' + \"_\".join( [str(v) for v in exp.values()] )+\".csv\" for exp in exp_lists_core] # file with JFI\n",
    "# jfi_files_edge = [data_dir + '/j_' + \"_\".join( [str(v) for v in exp.values()] )+\".csv\" for exp in exp_lists_edge] # file with JFI\n",
    "\n",
    "\n",
    "# #  First let us plot for Core \n",
    "# #Read each CSV file and append its DataFrame to the list and concatenate it later\n",
    "# dfs = []\n",
    "# for filename in jfi_files_core:\n",
    "#     df  = pd.read_csv(filename, header=0, names=['cca', 'duration','rtt','BW','sq_BW', 'flow_count', 'jfi'])\n",
    "#     dfs.append(df)  \n",
    "# jfi_dat_core = pd.concat(dfs, ignore_index=True) \n",
    "# cca_core=pd.unique(jfi_dat_core.cca)\n",
    "\n",
    "# with PdfPages(plots_dir + \"/JFI_plot_core.pdf\") as pdf:\n",
    "#     for i,c in enumerate(cca_core):\n",
    "#         plt.rcParams['figure.figsize'] = (5,3)\n",
    "#         plt.rcParams['axes.axisbelow'] = True\n",
    "#         plt.grid(axis='y')\n",
    "#         dat_cca=jfi_dat_core[(jfi_dat_core['cca'] == c)]\n",
    "#         dat_cca.sort_values(by=['flow_count'])\n",
    "#         print(dat_cca)\n",
    "#         flows=pd.unique(dat_cca.flow_count)\n",
    "#         rtt_num = pd.unique(dat_cca.rtt)       \n",
    "#         for j,r in enumerate(rtt_num):\n",
    "#             xvals[j] = dat_cca.jfi[jfi_dat_core['rtt']==r]\n",
    "#             bar[j] = plt.bar(ind + (width+0.02)*j, xvals[j], width, color = color_name[str(r)])\n",
    "            \n",
    "#         plt.xlabel(\"Flow Count\")\n",
    "#         plt.ylabel('JFI')\n",
    "#         plt.title(\"Core Scale, \"+\"CCA-\"+c)        \n",
    "#         plt.xticks(ind+width,flows)\n",
    "#         plt.legend( (bar[j] for j in range(len(rtt_num))), (str(rtt_num[j]) + \" ms\" for j in range(len(rtt_num))), bbox_to_anchor=(1, 0.5), frameon=False )   \n",
    "#         #pdf.savefig(bbox_inches=\"tight\")\n",
    "#         plt.show()\n",
    "#         plt.close()\n",
    "        \n",
    "# #  Now let us plot for Edge \n",
    "# #Read each CSV file and append its DataFrame to the list and concatenate it later\n",
    "# dfs = []\n",
    "# for filename in jfi_files_edge:\n",
    "#     df  = pd.read_csv(filename, header=0, names=['cca', 'duration','rtt','BW','sq_BW', 'flow_count', 'jfi'])\n",
    "#     dfs.append(df)  \n",
    "# jfi_dat_edge = pd.concat(dfs, ignore_index=True) \n",
    "# cca_edge=pd.unique(jfi_dat_edge.cca)\n",
    "\n",
    "# with PdfPages(plots_dir + \"/JFI_plot_edge.pdf\") as pdf:\n",
    "#     for i,c in enumerate(cca_edge):\n",
    "#         plt.rcParams['figure.figsize'] = (5,3)\n",
    "#         plt.rcParams['axes.axisbelow'] = True\n",
    "#         plt.grid(axis='y')\n",
    "#         dat_cca=jfi_dat_edge[(jfi_dat_edge['cca'] == c)]\n",
    "#         dat_cca.sort_values(by=['flow_count'])\n",
    "#         print(dat_cca)\n",
    "#         flows=pd.unique(dat_cca.flow_count)\n",
    "#         rtt_num = pd.unique(dat_cca.rtt)       \n",
    "#         for j,r in enumerate(rtt_num):\n",
    "#             xvals[j] = dat_cca.jfi[jfi_dat_edge['rtt']==r]\n",
    "#             bar[j] = plt.bar(ind + (width+0.02)*j, xvals[j], width, color = color_name[str(r)])\n",
    "            \n",
    "#         plt.xlabel(\"Flow Count\")\n",
    "#         plt.ylabel('JFI')\n",
    "#         plt.title(\"Edge Scale, \"+\"CCA-\"+c)        \n",
    "#         plt.xticks(ind+width,flows)\n",
    "#         plt.legend( (bar[j] for j in range(len(rtt_num))), (str(rtt_num[j]) + \" ms\" for j in range(len(rtt_num))), bbox_to_anchor=(1, 0.5), frameon=False )   \n",
    "#         #pdf.savefig(bbox_inches=\"tight\")\n",
    "#         plt.show()\n",
    "#         plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56551dcc-f9b7-4a74-b4e5-40052841705e",
   "metadata": {},
   "source": [
    "### Save the linear regression plots to a pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c3837",
   "metadata": {},
   "source": [
    "This should be placed within the experiment running cell to plot the Linear regression plots. Not required now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f842af0-b97b-4365-84b3-d74235381bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with PdfPages(\"linear_reg_plot.pdf\") as pdf:\n",
    "#   plt.rcParams['figure.figsize'] = (8,6)\n",
    "\n",
    "#   plt.scatter(x=x_retrans_ss, y=dat_exp['bitrate']*1000.0, color='C4', alpha=1, s=10, label='actual values')\n",
    "#   plt.scatter(x=x_retrans_ss, y=predicted_bw_ss, color='C3',  alpha=1, s=10, label='predicted_values')\n",
    "#   plt.plot(x_retrans_ss, predicted_bw_ss, color='C2', linewidth=0.5, label='fit')\n",
    "#   plt.xlabel(\"x=mss/rtt*sqrt(packet_loss_rate)\") \n",
    "#   plt.ylabel(\"y=bandwidth(bits/sec)\")\n",
    "#   plt.title(\"Method-1 calculation of packet_loss rate using data_seg_out from ss and retrans from ss data\")\n",
    "#   plt.legend()\n",
    "#   pdf.savefig()  # saves the current figure into a pdf page\n",
    "#   plt.show()\n",
    "#   plt.close()\n",
    "\n",
    "#   plt.scatter(x=x_retrans_iperf, y=dat_exp['bitrate']*1000.0, color='C4', s=10, label='actual values')\n",
    "#   plt.scatter(x=x_retrans_iperf, y=predicted_bw_iperf, color='C3', s=10, label='predicted_values')\n",
    "#   plt.plot(x_retrans_iperf, predicted_bw_iperf, color='C2', linewidth=0.5, label='fit')\n",
    "#   plt.xlabel(\"x=mss/rtt*sqrt(packet_loss_rate)\") \n",
    "#   plt.ylabel(\"y=bandwidth(bits/sec)\")\n",
    "#   plt.title(\"Method-2: calculation of packet_loss rate using data_seg_out from ss and retrans from iperf3 data\")\n",
    "#   plt.legend()\n",
    "#   pdf.savefig()  # saves the current figure into a pdf page\n",
    "#   plt.show()\n",
    "#   plt.close()\n",
    "\n",
    "\n",
    "#   plt.scatter(x=x_cwnd, y=dat_exp['bitrate']*1000.0, color='C4', alpha=1, s=10, label='actual values')\n",
    "#   plt.scatter(x=x_cwnd, y=predicted_bw_cwnd, color='C3',  alpha=1, s=10, label='predicted_values')\n",
    "#   plt.plot(x_cwnd, predicted_bw_cwnd, color='C2', linewidth=0.5, label='fit')\n",
    "#   plt.xlabel(\"x=mss/rtt*sqrt(packet_loss_rate)\")\n",
    "#   plt.ylabel(\"y=bandwidth(bits/sec)\")\n",
    "#   plt.title(\"Method-3: calculation of packet_loss rate using data_seg_out from ss and cwnd from iperf3 data\")\n",
    "#   plt.legend()\n",
    "#   pdf.savefig()  # saves the current figure into a pdf page\n",
    "#   plt.show()\n",
    "#   plt.close()\n",
    "\n",
    "#   plt.scatter(x=x_router, y=dat_exp['bitrate']*1000.0, color='C4', alpha=1, s=10, label='actual values')\n",
    "#   plt.scatter(x=x_router, y=predicted_bw_router, color='C3',  alpha=1, s=10, label='predicted_values')\n",
    "#   plt.plot(x_router, predicted_bw_router, color='C2', linewidth=0.5, label='fit')\n",
    "#   plt.xlabel(\"x=mss/rtt*sqrt(packet_loss_rate)\")\n",
    "#   plt.ylabel(\"y=bandwidth(bits/sec)\")\n",
    "#   plt.title(\"Method-4: calculation of packet_loss rate using packet drop rate at the router\")\n",
    "#   plt.legend()\n",
    "#   pdf.savefig()  # saves the current figure into a pdf page\n",
    "#   plt.show()\n",
    "#   plt.close()\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
